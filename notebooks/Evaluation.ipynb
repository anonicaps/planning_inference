{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from planning_inference.parsers import parse_model, parse_problem, parse_plan, parse_trajectory, parse_hypothesis, parse_observation_sequence\n",
    "from planning_inference.generator import generate_trajectory\n",
    "from planning_inference.functions import generate_all_literals, get_matching_literals\n",
    "\n",
    "from planning_inference.pddl import Conjunction, Literal, Type, TypedObject, Effect, Truth, NumericConstant, PrimitiveNumericExpression, Increase\n",
    "from planning_inference.pddl import SensorModel\n",
    "\n",
    "from planning_inference.observations import Trajectory, Hypothesis, State\n",
    "\n",
    "from planning_inference import DecodingTask\n",
    "\n",
    "from sensor_models import load_sensor_model\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from random import choice, choices, shuffle\n",
    "from statistics import mean\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(domain, task, table_format=False):\n",
    "\n",
    "    observabilities = [30, 50, 70]       \n",
    "    for observability in observabilities:\n",
    "        base_path = \"benchmark/%s/%s/%s/\" % (domain, task, str(observability))\n",
    "\n",
    "        problems = sorted(glob.glob(base_path + \"*\"))\n",
    "\n",
    "        H_sizes = []\n",
    "        H_star_sizes = []\n",
    "        found_correct = []\n",
    "        times = []\n",
    "\n",
    "        for problem in problems:\n",
    "            h_costs = []\n",
    "            h_times = []\n",
    "\n",
    "            with open(problem + \"/costs\", \"r\") as f:\n",
    "                h_costs = [int(c) for c in f.read().strip().split(\" \")]\n",
    "\n",
    "            with open(problem + \"/times\", \"r\") as f:\n",
    "                h_times = [float(t) for t in f.read().strip().split(\" \")]\n",
    "\n",
    "            # Real Hypothesis\n",
    "            with open(problem + \"/sol\", \"r\") as f:\n",
    "                correct_h = int(f.read())\n",
    "\n",
    "            # Hypotheses\n",
    "            h_files = sorted(glob.glob(problem + \"/hyp*\"))\n",
    "\n",
    "            H_sizes += [len(h_files)]\n",
    "\n",
    "            min_cost = min(h_costs)\n",
    "            H_star = [i for i in range(len(h_costs)) if h_costs[i] == min_cost]\n",
    "\n",
    "            H_star_sizes += [len(H_star)]\n",
    "\n",
    "            found_correct += [correct_h in H_star]\n",
    "\n",
    "            times += [sum(h_times)]\n",
    "\n",
    "        quality = len([found for found in found_correct if found])/len(found_correct)\n",
    "        \n",
    "        if table_format:\n",
    "            print(\"%.2f & %.2f & %.2f & %.2f\" % (mean(H_sizes), mean(H_star_sizes), quality, mean(times)))\n",
    "        else:\n",
    "            print(\"%s: %s at %s%%\" % (domain, task, str(observability)))\n",
    "            print(\"|H| = %.2f, |H*| = %.2f, Q = %.2f, T = %.2f\" % (mean(H_sizes), mean(H_star_sizes), quality, mean(times)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"openstacks\"\n",
    "table_format = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openstacks: monitoring at 30%\n",
      "|H| = 5.60, |H*| = 2.30, Q = 0.80, T = 5.93\n",
      "openstacks: monitoring at 50%\n",
      "|H| = 5.60, |H*| = 2.20, Q = 0.70, T = 6.81\n",
      "openstacks: monitoring at 70%\n",
      "|H| = 5.60, |H*| = 2.30, Q = 0.80, T = 9.60\n"
     ]
    }
   ],
   "source": [
    "# MONITORING\n",
    "\n",
    "task = \"monitoring\"\n",
    "evaluate(domain, task, table_format = table_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openstacks: hindsight at 30%\n",
      "|H| = 5.80, |H*| = 2.00, Q = 1.00, T = 11.19\n",
      "openstacks: hindsight at 50%\n",
      "|H| = 5.80, |H*| = 1.70, Q = 1.00, T = 11.32\n",
      "openstacks: hindsight at 70%\n",
      "|H| = 5.80, |H*| = 1.60, Q = 1.00, T = 14.28\n"
     ]
    }
   ],
   "source": [
    "# HINDSIGHT\n",
    "\n",
    "task = \"hindsight\"\n",
    "evaluate(domain, task, table_format = table_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openstacks: prediction at 30%\n",
      "|H| = 5.90, |H*| = 2.50, Q = 1.00, T = 15.70\n",
      "openstacks: prediction at 50%\n",
      "|H| = 5.90, |H*| = 2.50, Q = 1.00, T = 20.30\n",
      "openstacks: prediction at 70%\n",
      "|H| = 5.90, |H*| = 2.50, Q = 1.00, T = 18.77\n"
     ]
    }
   ],
   "source": [
    "# PREDICTION\n",
    "\n",
    "task = \"prediction\"\n",
    "evaluate(domain, task, table_format = table_format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
